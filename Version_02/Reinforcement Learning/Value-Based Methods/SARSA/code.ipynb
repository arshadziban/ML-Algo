{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b22c910",
   "metadata": {},
   "source": [
    "## **What is SARSA?**\n",
    "\n",
    "SARSA is a **reinforcement learning algorithm** similar to Q-learning, but it updates its values **using the action actually taken by the current policy** instead of the best possible future action.\n",
    "\n",
    "The name **SARSA** comes from the tuple it uses for updates:\n",
    "\n",
    "$$\n",
    "(S, A, R, S', A')\n",
    "$$\n",
    "\n",
    "* $S$ = current state\n",
    "* $A$ = action taken\n",
    "* $R$ = reward received\n",
    "* $S'$ = next state\n",
    "* $A'$ = next action chosen by **the same policy**\n",
    "\n",
    "---\n",
    "\n",
    "## **On-Policy Updates**\n",
    "\n",
    "* **On-policy** means the agent learns about the policy it is currently following.\n",
    "* In SARSA, if the policy is ε-greedy, both the action in the current state and the action in the next state are chosen **with ε-greedy**.\n",
    "* This makes SARSA’s updates reflect **the actual exploration** the agent does.\n",
    "\n",
    "---\n",
    "\n",
    "## **Update Rule**\n",
    "\n",
    "SARSA’s update equation is:\n",
    "\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S, A) + \\alpha \\big[ R + \\gamma Q(S', A') - Q(S, A) \\big]\n",
    "$$\n",
    "\n",
    "* Compare with Q-learning:\n",
    "\n",
    "  * Q-learning uses $\\max_{a'} Q(S', a')$ (best possible future action).\n",
    "  * SARSA uses $Q(S', A')$ for the **action the policy actually picks**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Difference from Q-learning**\n",
    "\n",
    "* **Q-learning** is **off-policy**: it learns as if it always takes the best action next time.\n",
    "* **SARSA** is **on-policy**: it learns about the actions it *really* takes, including exploratory ones.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tiny Example**\n",
    "\n",
    "Imagine the agent has a risky shortcut to the goal:\n",
    "\n",
    "* **Q-learning** might learn “always take shortcut” because it assumes you’ll handle it perfectly.\n",
    "* **SARSA** might learn “sometimes take safer route” because it updates using the actual ε-greedy moves, which include risky mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afaf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bbdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
