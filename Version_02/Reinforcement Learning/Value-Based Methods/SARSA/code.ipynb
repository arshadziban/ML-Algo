{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b22c910",
   "metadata": {},
   "source": [
    "## **What is SARSA?**\n",
    "\n",
    "SARSA is a **reinforcement learning algorithm** similar to Q-learning, but it updates its values **using the action actually taken by the current policy** instead of the best possible future action.\n",
    "\n",
    "The name **SARSA** comes from the tuple it uses for updates:\n",
    "\n",
    "$$\n",
    "(S, A, R, S', A')\n",
    "$$\n",
    "\n",
    "* $S$ = current state\n",
    "* $A$ = action taken\n",
    "* $R$ = reward received\n",
    "* $S'$ = next state\n",
    "* $A'$ = next action chosen by **the same policy**\n",
    "\n",
    "---\n",
    "\n",
    "## **On-Policy Updates**\n",
    "\n",
    "* **On-policy** means the agent learns about the policy it is currently following.\n",
    "* In SARSA, if the policy is ε-greedy, both the action in the current state and the action in the next state are chosen **with ε-greedy**.\n",
    "* This makes SARSA’s updates reflect **the actual exploration** the agent does.\n",
    "\n",
    "---\n",
    "\n",
    "## **Update Rule**\n",
    "\n",
    "SARSA’s update equation is:\n",
    "\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S, A) + \\alpha \\big[ R + \\gamma Q(S', A') - Q(S, A) \\big]\n",
    "$$\n",
    "\n",
    "* Compare with Q-learning:\n",
    "\n",
    "  * Q-learning uses $\\max_{a'} Q(S', a')$ (best possible future action).\n",
    "  * SARSA uses $Q(S', A')$ for the **action the policy actually picks**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Difference from Q-learning**\n",
    "\n",
    "* **Q-learning** is **off-policy**: it learns as if it always takes the best action next time.\n",
    "* **SARSA** is **on-policy**: it learns about the actions it *really* takes, including exploratory ones.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tiny Example**\n",
    "\n",
    "Imagine the agent has a risky shortcut to the goal:\n",
    "\n",
    "* **Q-learning** might learn “always take shortcut” because it assumes you’ll handle it perfectly.\n",
    "* **SARSA** might learn “sometimes take safer route” because it updates using the actual ε-greedy moves, which include risky mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12afaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[[ 1.9   3.18]\n",
      " [ 1.21  5.71]\n",
      " [ 1.09  6.08]\n",
      " [ 2.11 10.  ]\n",
      " [ 0.    0.  ]]\n",
      "\n",
      "Greedy policy by state (0=left, 1=right):\n",
      "state 0: 1 (right)\n",
      "state 1: 1 (right)\n",
      "state 2: 1 (right)\n",
      "state 3: 1 (right)\n",
      "state 4: 0 (left)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# --- Tiny 1D world: states 0..4 (4 is terminal/goal) ---\n",
    "n_states = 5\n",
    "actions = {0:\"left\", 1:\"right\"}\n",
    "n_actions = len(actions)\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Environment dynamics.\"\"\"\n",
    "    if state == 4:                      # terminal\n",
    "        return state, 0, True\n",
    "    if action == 0:                     # left\n",
    "        next_state = max(0, state-1)\n",
    "    else:                               # right\n",
    "        next_state = min(4, state+1)\n",
    "    reward = 10 if next_state == 4 else -1\n",
    "    done = (next_state == 4)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# --- SARSA hyperparams ---\n",
    "alpha  = 0.5      # learning rate\n",
    "gamma  = 0.9      # discount factor\n",
    "epsilon = 0.2     # ε-greedy exploration\n",
    "episodes = 300\n",
    "\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "def epsilon_greedy(state, eps):\n",
    "    \"\"\"Pick action with ε-greedy policy from Q.\"\"\"\n",
    "    if rng.random() < eps:\n",
    "        return rng.integers(n_actions)           # explore\n",
    "    # break ties randomly to avoid sticking to action 0\n",
    "    best = np.flatnonzero(Q[state] == Q[state].max())\n",
    "    return rng.choice(best)\n",
    "\n",
    "# --- SARSA training loop ---\n",
    "for ep in range(episodes):\n",
    "    s = 0\n",
    "    a = epsilon_greedy(s, epsilon)\n",
    "    done = False\n",
    "    while not done:\n",
    "        s_next, r, done = step(s, a)\n",
    "        if not done:\n",
    "            a_next = epsilon_greedy(s_next, epsilon)    # on-policy next action\n",
    "            td_target = r + gamma * Q[s_next, a_next]\n",
    "        else:\n",
    "            a_next = None\n",
    "            td_target = r                               # terminal; no bootstrap\n",
    "        td_error = td_target - Q[s, a]\n",
    "        Q[s, a] += alpha * td_error\n",
    "\n",
    "        s, a = s_next, (a_next if a_next is not None else 0)\n",
    "\n",
    "# --- Show learned Q and policy ---\n",
    "print(\"Q-table:\")\n",
    "print(np.round(Q, 2))\n",
    "\n",
    "policy = np.argmax(Q, axis=1)\n",
    "print(\"\\nGreedy policy by state (0=left, 1=right):\")\n",
    "for s in range(n_states):\n",
    "    print(f\"state {s}: {policy[s]} ({actions[policy[s]]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bbdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
