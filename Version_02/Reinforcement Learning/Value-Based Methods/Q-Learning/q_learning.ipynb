{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e29e3d",
   "metadata": {},
   "source": [
    "**Q-learning** is a **reinforcement learning algorithm** that teaches an agent to choose the best action in each situation by learning a table of values (Q-table) through trial and error.\n",
    "Each value in the table estimates **how good** a specific action is in a given state, based on the rewards the agent gets now and in the future.\n",
    "\n",
    "**Why we use it:**\n",
    "\n",
    "* To find the **optimal policy** (best way to act) without knowing the environmentâ€™s rules in advance.\n",
    "* Works well in problems where the agent learns by **interacting** with the environment and receiving feedback.\n",
    "* Useful for games, robot navigation, decision-making systems, and any situation where you want to **maximize long-term rewards**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf6582",
   "metadata": {},
   "source": [
    "## **Mathematical Definition**\n",
    "\n",
    "We want to learn the **optimal action-value function**:\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\mathbb{E} \\left[ r_t + \\gamma \\max_{a'} Q^*(s_{t+1},a') \\right]\n",
    "$$\n",
    "\n",
    "We use the **update rule**:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\big]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $s$ = current state\n",
    "* $a$ = chosen action\n",
    "* $r$ = immediate reward\n",
    "* $s'$ = next state\n",
    "* $\\alpha$ = learning rate\n",
    "* $\\gamma$ = discount factor\n",
    "\n",
    "---\n",
    "\n",
    "## **Tiny Example Dataset**\n",
    "\n",
    "We have 2 states and 2 actions:\n",
    "\n",
    "| State | Action | Next State | Reward |\n",
    "| ----- | ------ | ---------- | ------ |\n",
    "| S1    | Right  | S2         | 1      |\n",
    "| S2    | Right  | Terminal   | 5      |\n",
    "| S1    | Left   | S1         | 0      |\n",
    "| S2    | Left   | S1         | 0      |\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* $\\alpha = 0.5$\n",
    "* $\\gamma = 0.9$\n",
    "* Q-table starts at 0\n",
    "\n",
    "---\n",
    "\n",
    "### **First Update**\n",
    "\n",
    "From S1, take **Right** â†’ S2, reward = 1\n",
    "\n",
    "$$\n",
    "Q(S1,\\text{Right}) = 0 + 0.5 \\times [1 + 0.9 \\times \\max_{a'}Q(S2,a') - 0]\n",
    "$$\n",
    "\n",
    "At the start, $Q(S2,*) = 0$:\n",
    "\n",
    "$$\n",
    "Q(S1,\\text{Right}) = 0.5 \\times [1 + 0 - 0] = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Second Update**\n",
    "\n",
    "From S2, take **Right** â†’ Terminal, reward = 5\n",
    "\n",
    "$$\n",
    "Q(S2,\\text{Right}) = 0 + 0.5 \\times [5 + 0.9 \\times 0 - 0]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(S2,\\text{Right}) = 2.5\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da08439",
   "metadata": {},
   "source": [
    "## **Example: Warehouse Robot Navigation**\n",
    "\n",
    "**Scenario:**\n",
    "A company has a robot that moves in a warehouse to pick up packages.\n",
    "The robot must learn the **best path** from the charging station to the package location while **avoiding obstacles**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How an ML Engineer Applies Q-Learning**\n",
    "\n",
    "1. **Define the states (S)**\n",
    "   Each possible position in the warehouse grid is a state.\n",
    "\n",
    "2. **Define the actions (A)**\n",
    "\n",
    "   * Move Up, Move Down, Move Left, Move Right\n",
    "\n",
    "3. **Define the rewards (R)**\n",
    "\n",
    "   * **+10** for reaching the package\n",
    "   * **âˆ’10** for hitting an obstacle\n",
    "   * **âˆ’1** for each move (to encourage shorter paths)\n",
    "\n",
    "4. **Initialize the Q-table**\n",
    "   Rows = positions, columns = possible moves.\n",
    "   Initially, all values = 0.\n",
    "\n",
    "5. **Run training episodes**\n",
    "\n",
    "   * The robot starts in a random spot.\n",
    "   * Chooses actions with **Îµ-greedy** policy.\n",
    "   * Moves, gets rewards, updates the Q-table using the **Bellman equation**.\n",
    "\n",
    "6. **Convergence**\n",
    "   Over time, the Q-values converge so that **the best move from each position is clear**.\n",
    "\n",
    "7. **Deployment**\n",
    "\n",
    "   * Once trained, the robot just **looks up the best action in the Q-table** for its current position.\n",
    "   * No need for human control â€” it navigates optimally.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **Why Q-Learning here?**\n",
    "\n",
    "* The engineer doesnâ€™t need to know the exact warehouse map in advance.\n",
    "* The robot learns from **trial and error** by exploring and adjusting.\n",
    "* Works well even if obstacles change over time â€” the robot can keep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86abf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      " [[ 3.12  4.58]\n",
      " [ 3.12  6.2 ]\n",
      " [ 4.58  8.  ]\n",
      " [ 6.2  10.  ]\n",
      " [ 0.    0.  ]]\n",
      "Greedy policy by state: ['right', 'right', 'right', 'right', 'terminal']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# --- Tiny 1D world: states 0..4 (4 is terminal/goal) ---\n",
    "n_states = 5\n",
    "actions = {0:\"left\", 1:\"right\"}\n",
    "n_actions = len(actions)\n",
    "\n",
    "# Rewards: -1 per step, +10 on reaching goal\n",
    "def step(state, action):\n",
    "    if state == 4:  # terminal\n",
    "        return state, 0, True\n",
    "    if action == 0:  # left\n",
    "        next_state = max(0, state-1)\n",
    "    else:            # right\n",
    "        next_state = min(4, state+1)\n",
    "    reward = 10 if next_state == 4 else -1\n",
    "    done = (next_state == 4)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# --- Q-learning hyperparams ---\n",
    "alpha = 0.5       # learning rate\n",
    "gamma = 0.9       # discount\n",
    "epsilon = 0.2     # Îµ-greedy exploration\n",
    "episodes = 200\n",
    "\n",
    "Q = np.zeros((n_states, n_actions))  # Q-table\n",
    "\n",
    "for _ in range(episodes):\n",
    "    s = 0  # start\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Îµ-greedy action\n",
    "        if rng.random() < epsilon:\n",
    "            a = rng.integers(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "\n",
    "        s_next, r, done = step(s, a)\n",
    "\n",
    "        # Bellman update\n",
    "        best_next = 0 if done else np.max(Q[s_next])\n",
    "        td_target = r + gamma * best_next\n",
    "        Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "# Show learned Q and greedy policy\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\"Q-table:\\n\", Q)\n",
    "\n",
    "greedy_policy = [actions[np.argmax(Q[s])] if s != 4 else \"terminal\" for s in range(n_states)]\n",
    "print(\"Greedy policy by state:\", greedy_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7a13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
