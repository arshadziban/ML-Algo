{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c378f1",
   "metadata": {},
   "source": [
    "## 1. Gaussian Mixture Models (GMM)\n",
    "\n",
    "* **What is GMM?**\n",
    "  It is a way to group data by assuming that the data comes from several groups (clusters), each shaped like a bell curve (Gaussian/normal distribution).\n",
    "\n",
    "* **Why use GMM?**\n",
    "  Unlike K-Means that puts each data point into only one group, GMM says each data point can belong to several groups at the same time, but with different probabilities.\n",
    "\n",
    "* **How does it work?**\n",
    "  The data’s overall shape is made by mixing several bell curves. Each curve has its own center (mean), shape (covariance), and weight (how big it is).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "* **What is EM?**\n",
    "  It’s a step-by-step method to find the best guess for the parameters (centers, shapes, and weights) of those bell curves.\n",
    "\n",
    "* **How EM works in GMM:**\n",
    "\n",
    "  1. **Start:** Guess the centers, shapes, and weights for each group.\n",
    "  2. **E-step:** For each data point, calculate how likely it belongs to each group (soft assignment).\n",
    "  3. **M-step:** Update the centers, shapes, and weights based on these likelihoods.\n",
    "  4. **Repeat** these two steps until the parameters don’t change much.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Soft Clustering\n",
    "\n",
    "* **What is soft clustering?**\n",
    "  Instead of forcing a point into just one cluster, soft clustering says a point can belong to multiple clusters with some probability.\n",
    "\n",
    "* **Why is it useful?**\n",
    "  It helps when groups overlap or when we aren’t sure which group a point belongs to. It gives more information about the data structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Summary Table\n",
    "\n",
    "| Concept                  | What it Means                                | Main Idea                                   |\n",
    "| ------------------------ | -------------------------------------------- | ------------------------------------------- |\n",
    "| Gaussian Mixture Model   | Data comes from several bell curves          | Mix of multiple Gaussian distributions      |\n",
    "| Expectation-Maximization | Step-by-step way to find best groups         | Calculate probabilities, then update groups |\n",
    "| Soft Clustering          | Points belong to clusters with probabilities | Probabilistic membership, not hard labels   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data and initialization\n",
    "y = np.array([1.0, 1.5, 2.0, 5.0, 6.0, 6.5])\n",
    "K = 2\n",
    "np.random.seed(0)\n",
    "pi = np.array([0.5, 0.5])\n",
    "mu = np.array([1.0, 5.0])\n",
    "sigma2 = np.array([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2600a4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted cluster probabilities for x=4.0: [8.27444470e-07 9.99999173e-01]\n"
     ]
    }
   ],
   "source": [
    "def gaussian_pdf(x, mean, var):\n",
    "    return (1/np.sqrt(2 * np.pi * var)) * np.exp(-(x - mean)**2 / (2 * var))\n",
    "\n",
    "def em_step(y, pi, mu, sigma2):\n",
    "    N = len(y)\n",
    "    K = len(pi)\n",
    "    gamma = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        denom = 0\n",
    "        for k in range(K):\n",
    "            gamma[i, k] = pi[k] * gaussian_pdf(y[i], mu[k], sigma2[k])\n",
    "            denom += gamma[i, k]\n",
    "        gamma[i, :] /= denom\n",
    "    N_k = np.sum(gamma, axis=0)\n",
    "    for k in range(K):\n",
    "        mu[k] = np.sum(gamma[:, k] * y) / N_k[k]\n",
    "        sigma2[k] = np.sum(gamma[:, k] * (y - mu[k])**2) / N_k[k]\n",
    "        pi[k] = N_k[k] / N\n",
    "    return pi, mu, sigma2, gamma\n",
    "\n",
    "# Train for 5 iterations\n",
    "for _ in range(5):\n",
    "    pi, mu, sigma2, gamma = em_step(y, pi, mu, sigma2)\n",
    "\n",
    "# New data point to predict\n",
    "x_new = 4.0\n",
    "\n",
    "# Calculate responsibilities for the new point\n",
    "probs = np.array([pi[k] * gaussian_pdf(x_new, mu[k], sigma2[k]) for k in range(K)])\n",
    "probs /= probs.sum()  # normalize to sum to 1\n",
    "\n",
    "print(f\"Predicted cluster probabilities for x={x_new}: {probs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
