{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c378f1",
   "metadata": {},
   "source": [
    "## 1. Gaussian Mixture Models (GMM)\n",
    "\n",
    "* **What is GMM?**\n",
    "  A Gaussian Mixture Model is a probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions (normal distributions) with unknown parameters.\n",
    "\n",
    "* **Why use GMM?**\n",
    "  Unlike K-Means, which assigns each data point to exactly one cluster (hard clustering), GMM allows for probabilistic (soft) assignment, meaning each point belongs to each cluster with some probability.\n",
    "\n",
    "* **Mathematical Formulation:**\n",
    "  Suppose you have $K$ clusters. The probability density function for a data point $x$ is:\n",
    "\n",
    "  $$\n",
    "  p(x) = \\sum_{k=1}^K \\pi_k \\cdot \\mathcal{N}(x|\\mu_k, \\Sigma_k)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  * $\\pi_k$ is the mixing coefficient for cluster $k$, with $\\sum_{k=1}^K \\pi_k = 1$ and $\\pi_k \\geq 0$\n",
    "  * $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ is the Gaussian distribution with mean $\\mu_k$ and covariance matrix $\\Sigma_k$\n",
    "\n",
    "* **Parameters to estimate:**\n",
    "\n",
    "  * Means $\\mu_k$\n",
    "  * Covariances $\\Sigma_k$\n",
    "  * Mixing weights $\\pi_k$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "* **Purpose:**\n",
    "  EM is an iterative method to find the maximum likelihood estimates of parameters in models with latent variables (hidden data), like GMM where the cluster assignment is hidden.\n",
    "\n",
    "* **How EM works in GMM?**\n",
    "  The cluster assignments (which Gaussian generated each point) are unknown latent variables.\n",
    "\n",
    "* **Steps:**\n",
    "\n",
    "  **Initialization:**\n",
    "  Start with initial guesses for $\\pi_k$, $\\mu_k$, and $\\Sigma_k$.\n",
    "\n",
    "  **E-step (Expectation):**\n",
    "  Calculate the probability (responsibility) that cluster $k$ generated data point $x_i$:\n",
    "\n",
    "  $$\n",
    "  \\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}\n",
    "  $$\n",
    "\n",
    "  This is the \"soft\" assignment of point $i$ to cluster $k$.\n",
    "\n",
    "  **M-step (Maximization):**\n",
    "  Update the parameters using these responsibilities:\n",
    "\n",
    "  $$\n",
    "  N_k = \\sum_{i=1}^N \\gamma_{ik}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\pi_k = \\frac{N_k}{N}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\mu_k = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} x_i\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "  $$\n",
    "\n",
    "* **Repeat** E and M steps until convergence (parameters stabilize or likelihood stops increasing).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Soft Clustering\n",
    "\n",
    "* **What is soft clustering?**\n",
    "  In soft clustering, each data point has a probability of belonging to each cluster instead of being assigned to only one cluster. This contrasts with hard clustering (like K-Means).\n",
    "\n",
    "* **Soft Clustering in GMM:**\n",
    "  The responsibilities $\\gamma_{ik}$ computed in the E-step are exactly soft assignments â€” each point belongs to each cluster with some fractional responsibility (probability).\n",
    "\n",
    "* **Benefits of Soft Clustering:**\n",
    "\n",
    "  * Models uncertainty about cluster membership.\n",
    "  * Can handle overlapping clusters better.\n",
    "  * Provides richer information about the structure of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Concept                  | Description                                     | Key Idea                                              |\n",
    "| ------------------------ | ----------------------------------------------- | ----------------------------------------------------- |\n",
    "| Gaussian Mixture Model   | Data modeled as a mix of Gaussian distributions | Mixture of Gaussians                                  |\n",
    "| Expectation-Maximization | Iterative algorithm to estimate parameters      | E-step (soft assignments), M-step (parameter updates) |\n",
    "| Soft Clustering          | Probabilistic cluster assignment                | Membership probabilities instead of hard labels       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24181078",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
