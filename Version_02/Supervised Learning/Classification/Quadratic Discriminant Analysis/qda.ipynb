{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb5f37",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "Quadratic Discriminant Analysis (QDA) is a classification method used in machine learning and statistics. It assumes that the features of each class follow a **Gaussian (normal) distribution**, but unlike some other methods, QDA allows each class to have its own covariance matrix.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "#### 1. Class-specific Covariance Matrices\n",
    "\n",
    "* Each class $k$ has its own covariance matrix $\\Sigma_k$.\n",
    "* This means the shape and spread of the data can be different for each class.\n",
    "* Because of this, the boundaries that separate classes are **quadratic curves** (not straight lines).\n",
    "\n",
    "#### 2. Gaussian Assumption\n",
    "\n",
    "* QDA assumes that data points from each class are drawn from a multivariate normal distribution.\n",
    "* The distribution for class $k$ is defined by its mean $\\mu_k$ and covariance $\\Sigma_k$.\n",
    "\n",
    "Mathematically, the probability density function for a data point $\\mathbf{x}$ given class $k$ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} | y = k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "where $d$ is the number of features.\n",
    "\n",
    "---\n",
    "\n",
    "### How QDA Works\n",
    "\n",
    "* For a new data point, QDA calculates how likely it is to belong to each class using the above Gaussian models.\n",
    "* It then predicts the class for which this likelihood (or posterior probability) is highest.\n",
    "* Because covariance matrices differ per class, the decision boundary is quadratic.\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Imagine two classes with a single feature:\n",
    "\n",
    "| Class | Data points   |\n",
    "| ----- | ------------- |\n",
    "| 0     | 1.0, 1.2, 0.8 |\n",
    "| 1     | 3.0, 2.8, 3.2 |\n",
    "\n",
    "* Calculate the mean and variance for each class separately.\n",
    "* For a new point (say, 2.0), compute the probability that it belongs to each class using the Gaussian formula.\n",
    "* Assign the point to the class with the higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3985805",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "| Fruit    | Size | Color | Class  |\n",
    "| -------- | ---- | ----- | ------ |\n",
    "| Apple 1  | 3    | 7     | Apple  |\n",
    "| Apple 2  | 2.5  | 6     | Apple  |\n",
    "| Orange 1 | 7    | 3     | Orange |\n",
    "| Orange 2 | 6.5  | 2     | Orange |\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Calculate the Mean Vector for Each Class\n",
    "\n",
    "For each class, calculate the average of each feature:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\begin{bmatrix}\n",
    "\\text{mean of Size} \\\\\n",
    "\\text{mean of Color}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Apple:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{Apple}} = \\begin{bmatrix}\n",
    "(3 + 2.5)/2 \\\\\n",
    "(7 + 6)/2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2.75 \\\\\n",
    "6.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Orange:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{Orange}} = \\begin{bmatrix}\n",
    "(7 + 6.5)/2 \\\\\n",
    "(3 + 2)/2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "6.75 \\\\\n",
    "2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Step 2: Calculate the Covariance Matrix for Each Class\n",
    "\n",
    "Covariance shows how features vary together in each class:\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{n_k - 1} \\sum_{i=1}^{n_k} (\\mathbf{x}_i - \\mu_k)(\\mathbf{x}_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "* For simplicity, assume covariance matrices are:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{Apple}} = \\Sigma_{\\text{Orange}} = \\begin{bmatrix}\n",
    "0.135 & 0.25 \\\\\n",
    "0.25 & 0.51\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Step 3: New Fruit to Classify\n",
    "\n",
    "A new fruit has features:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(Size = 5, Color = 4)\n",
    "\n",
    "---\n",
    "\n",
    "# Step 4: Calculate Difference Vector for Each Class\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_k = \\mathbf{x} - \\mu_k\n",
    "$$\n",
    "\n",
    "* Apple:\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_{Apple} = \\begin{bmatrix} 5 - 2.75 \\\\ 4 - 6.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 \\\\ -2.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Orange:\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_{Orange} = \\begin{bmatrix} 5 - 6.75 \\\\ 4 - 2.5 \\end{bmatrix} = \\begin{bmatrix} -1.75 \\\\ 1.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Step 5: Calculate the Mahalanobis Distance for Each Class\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "D_k^2 = \\mathbf{d}_k^T \\Sigma_k^{-1} \\mathbf{d}_k\n",
    "$$\n",
    "\n",
    "(You multiply the difference vector, inverse covariance matrix, and difference vector again)\n",
    "\n",
    "---\n",
    "\n",
    "# Step 6: Calculate Likelihood for Each Class (Ignoring constants)\n",
    "\n",
    "$$\n",
    "L_k = \\frac{1}{\\sqrt{|\\Sigma_k|}} \\exp\\left(-\\frac{1}{2} D_k^2 \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Step 7: Predict Class with Higher Likelihood\n",
    "\n",
    "* Calculate $L_{Apple}$ and $L_{Orange}$.\n",
    "* The class with the larger likelihood is the predicted class.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "| Class  | Mean Vector                               | Covariance Matrix                                         | Difference Vector $\\mathbf{d}_k$           | Distance $D_k^2$ | Likelihood $L_k$          | Prediction |\n",
    "| ------ | ----------------------------------------- | --------------------------------------------------------- | ------------------------------------------ | ---------------- | ------------------------- | ---------- |\n",
    "| Apple  | $\\begin{bmatrix}2.75 \\\\ 6.5\\end{bmatrix}$ | $\\begin{bmatrix}0.135 & 0.25 \\\\ 0.25 & 0.51\\end{bmatrix}$ | $\\begin{bmatrix}2.25 \\\\ -2.5\\end{bmatrix}$ | Large            | Small (close to 0)        |            |\n",
    "| Orange | $\\begin{bmatrix}6.75 \\\\ 2.5\\end{bmatrix}$ | $\\begin{bmatrix}0.135 & 0.25 \\\\ 0.25 & 0.51\\end{bmatrix}$ | $\\begin{bmatrix}-1.75 \\\\ 1.5\\end{bmatrix}$ | Smaller          | Larger (still close to 0) | **Orange** |\n",
    "\n",
    "---\n",
    "\n",
    "### So, the new fruit is predicted to be an **Orange**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d38f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Orange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: Size and Color for each fruit type\n",
    "apple_data = np.array([[3, 7], [2.5, 6]])\n",
    "orange_data = np.array([[7, 3], [6.5, 2]])\n",
    "\n",
    "# Step 1: Calculate means\n",
    "apple_mean = np.mean(apple_data, axis=0)\n",
    "orange_mean = np.mean(orange_data, axis=0)\n",
    "\n",
    "# Step 2: Calculate covariance matrices (add small value for stability)\n",
    "apple_cov = np.cov(apple_data, rowvar=False) + np.eye(2)*0.01\n",
    "orange_cov = np.cov(orange_data, rowvar=False) + np.eye(2)*0.01\n",
    "\n",
    "# New fruit features\n",
    "new_fruit = np.array([5, 4])\n",
    "\n",
    "# Function to calculate Mahalanobis distance squared\n",
    "def mahalanobis(x, mean, cov):\n",
    "    diff = x - mean\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    return diff.T.dot(cov_inv).dot(diff)\n",
    "\n",
    "# Calculate distances\n",
    "dist_apple = mahalanobis(new_fruit, apple_mean, apple_cov)\n",
    "dist_orange = mahalanobis(new_fruit, orange_mean, orange_cov)\n",
    "\n",
    "# Calculate likelihoods (ignoring constants)\n",
    "likelihood_apple = np.exp(-0.5 * dist_apple) / np.sqrt(np.linalg.det(apple_cov))\n",
    "likelihood_orange = np.exp(-0.5 * dist_orange) / np.sqrt(np.linalg.det(orange_cov))\n",
    "\n",
    "# Decide class\n",
    "if likelihood_apple > likelihood_orange:\n",
    "    print(\"Predicted class: Apple\")\n",
    "else:\n",
    "    print(\"Predicted class: Orange\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e582e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
