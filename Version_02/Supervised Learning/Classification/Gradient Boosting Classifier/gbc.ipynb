{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d220c03d",
   "metadata": {},
   "source": [
    "## **Gradient Boosting Classifier: Overview**\n",
    "\n",
    "Gradient Boosting is a powerful machine learning technique that builds a strong classifier by combining many weak learners (typically decision trees) **in sequence**.\n",
    "At each stage, the new tree tries to fix the errors made by the previous combined model.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Stage-wise Tree Boosting**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * The model starts with a simple prediction (like the average value).\n",
    "  * At each stage, a new tree is trained to predict the “residuals” (the errors made so far).\n",
    "  * These trees are added one by one, and each tries to correct the mistakes of the combined model so far.\n",
    "* **Formula (simplified):**\n",
    "  After $m$ stages (trees), the prediction is:\n",
    "\n",
    "  $$\n",
    "  F_m(x) = F_{m-1}(x) + \\text{learning rate} \\times \\text{Tree}_m(x)\n",
    "  $$\n",
    "\n",
    "  where $\\text{Tree}_m(x)$ is the prediction from the $m$-th tree.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Learning Rate**\n",
    "\n",
    "* The **learning rate** (also called “shrinkage”) is a number between 0 and 1 that **controls how much each new tree’s prediction affects the overall model**.\n",
    "* **Low learning rate:**\n",
    "\n",
    "  * Each tree has less influence.\n",
    "  * Needs more trees to fit well, but reduces the risk of overfitting.\n",
    "* **High learning rate:**\n",
    "\n",
    "  * Each tree has more influence.\n",
    "  * Model can learn faster, but may overfit.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Overfitting Control**\n",
    "\n",
    "Gradient boosting is **prone to overfitting** if not controlled.\n",
    "**Common strategies:**\n",
    "\n",
    "* **Use a low learning rate** (common values: 0.01 to 0.2)\n",
    "* **Limit the number of trees (n\\_estimators)**\n",
    "* **Restrict tree depth** (e.g., max\\_depth=3)\n",
    "* **Use subsampling** (randomly use a subset of data for each tree)\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Concept          | Description                                                |\n",
    "| ---------------- | ---------------------------------------------------------- |\n",
    "| Stage-wise       | Trees are added one at a time, correcting previous errors  |\n",
    "| Learning Rate    | Controls the impact of each tree (lower is safer, slower)  |\n",
    "| Overfitting Ctrl | Fewer trees, shallow trees, low learning rate, subsampling |\n",
    "\n",
    "---\n",
    "\n",
    "## **Mini Example**\n",
    "\n",
    "Suppose we want to classify emails as spam/not spam:\n",
    "\n",
    "1. **Stage 1:** Start with a baseline guess.\n",
    "2. **Stage 2:** Fit a small tree to the errors (residuals).\n",
    "3. **Stage 3:** Fit another tree to the new errors, and so on.\n",
    "4. **Learning rate:** If set to 0.1, each new tree’s correction is scaled down by 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50073227",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "\n",
    "| Sample | X | True Class (y) |\n",
    "| ------ | - | -------------- |\n",
    "| 1      | 1 | 0              |\n",
    "| 2      | 2 | 0              |\n",
    "| 3      | 3 | 1              |\n",
    "| 4      | 4 | 1              |\n",
    "\n",
    "Suppose we want to classify $X$ as 0 or 1 (binary classification).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Initial Prediction**\n",
    "\n",
    "We start with a simple guess, such as the average of the target values:\n",
    "\n",
    "$$\n",
    "F_0(x) = \\text{mean}(y) = \\frac{0 + 0 + 1 + 1}{4} = 0.5\n",
    "$$\n",
    "\n",
    "So, for every $x$, $F_0(x) = 0.5$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute Residuals**\n",
    "\n",
    "Residuals = True value - Current prediction\n",
    "\n",
    "| Sample | True y | F₀(x) | Residual (y - F₀(x)) |\n",
    "| ------ | ------ | ----- | -------------------- |\n",
    "| 1      | 0      | 0.5   | -0.5                 |\n",
    "| 2      | 0      | 0.5   | -0.5                 |\n",
    "| 3      | 1      | 0.5   | +0.5                 |\n",
    "| 4      | 1      | 0.5   | +0.5                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Fit First Tree on Residuals**\n",
    "\n",
    "Suppose a simple decision tree (stump) predicts:\n",
    "\n",
    "$$\n",
    "h_1(x) = \\begin{cases}\n",
    "-0.5 & \\text{if } x < 3 \\\\\n",
    "+0.5 & \\text{if } x \\geq 3\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Update Prediction (Use Learning Rate η = 0.1)**\n",
    "\n",
    "$$\n",
    "F_1(x) = F_0(x) + \\eta \\cdot h_1(x)\n",
    "$$\n",
    "\n",
    "For $x = 2$:\n",
    "\n",
    "$$\n",
    "F_1(2) = 0.5 + 0.1 \\times (-0.5) = 0.5 - 0.05 = 0.45\n",
    "$$\n",
    "\n",
    "For $x = 4$:\n",
    "\n",
    "$$\n",
    "F_1(4) = 0.5 + 0.1 \\times (0.5) = 0.5 + 0.05 = 0.55\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Continue Boosting**\n",
    "\n",
    "* Compute new residuals using $F_1(x)$\n",
    "* Fit the next tree on these new residuals\n",
    "* Update prediction: $F_2(x) = F_1(x) + \\eta \\cdot h_2(x)$\n",
    "* Repeat for more stages\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Sample | True y | F₀(x) | h₁(x) | F₁(x) |\n",
    "| ------ | ------ | ----- | ----- | ----- |\n",
    "| 1      | 0      | 0.5   | -0.5  | 0.45  |\n",
    "| 2      | 0      | 0.5   | -0.5  | 0.45  |\n",
    "| 3      | 1      | 0.5   | +0.5  | 0.55  |\n",
    "| 4      | 1      | 0.5   | +0.5  | 0.55  |\n",
    "\n",
    "---\n",
    "\n",
    "**In practice**, more trees (stages) and possibly deeper trees are added, but the logic remains the same:\n",
    "\n",
    "* Start with a simple prediction\n",
    "* Add trees that fit the residuals\n",
    "* Scale each tree by the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502f2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Initial prediction (F0): [0.5, 0.5, 0.5, 0.5]\n",
      "Step 2: Residuals: [-0.5, -0.5, 0.5, 0.5]\n",
      "Step 3: First tree predictions: [-0.5, -0.5, 0.5, 0.5]\n",
      "Step 4: Updated predictions (F1): [0.45, 0.45, 0.55, 0.55]\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "X = [1, 2, 3, 4]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "\n",
    "# Step 1: Initial prediction (mean of y)\n",
    "F0 = sum(y) / len(y)  # 0.5\n",
    "F = [F0, F0, F0, F0]\n",
    "print(\"Step 1: Initial prediction (F0):\", F)\n",
    "\n",
    "# Step 2: Residuals (true y - current prediction)\n",
    "residuals = [y[i] - F[i] for i in range(4)]\n",
    "print(\"Step 2: Residuals:\", residuals)\n",
    "\n",
    "# Step 3: First tree (manual stump)\n",
    "# Predict -0.5 for x<3, +0.5 for x>=3\n",
    "h1 = []\n",
    "for i in range(4):\n",
    "    if X[i] < 3:\n",
    "        h1.append(-0.5)\n",
    "    else:\n",
    "        h1.append(0.5)\n",
    "print(\"Step 3: First tree predictions:\", h1)\n",
    "\n",
    "# Step 4: Update predictions with learning rate\n",
    "F1 = [F[i] + eta * h1[i] for i in range(4)]\n",
    "print(\"Step 4: Updated predictions (F1):\", F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
