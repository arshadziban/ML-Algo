{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395b0cf2",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge Regression is like **regular linear regression with a \"safety brake\"**. It prevents the model from getting too excited and fitting the data too closely.\n",
    "\n",
    "Think of it this way:\n",
    "\n",
    "* **Regular linear regression** tries to draw the perfect line through all data points\n",
    "* **Ridge regression** draws a good line but also keeps the line from getting too steep or wiggly\n",
    "\n",
    "---\n",
    "\n",
    "## What is L2 Regularization? (The Key Ingredient!)\n",
    "\n",
    "**L2 regularization** is the special rule used in Ridge Regression.\n",
    "\n",
    "* It adds a **penalty** based on the **sum of the squares of the model’s weights** (coefficients).\n",
    "* This penalty is called the **L2 penalty** because of the squared numbers (the “2” in L2).\n",
    "\n",
    "### L2 in the Ridge Formula\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\text{Prediction Error} + \\lambda \\cdot \\left(\\text{sum of squared weights}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\lambda$ (lambda) is how strong you want the penalty to be (like the speed limit).\n",
    "* The penalty is the **L2 term**: if your weights are big, the penalty is big; if weights are small, the penalty is small.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need Ridge Regression?\n",
    "\n",
    "### Problem: Overfitting\n",
    "\n",
    "Imagine you're trying to predict house prices. Regular linear regression might:\n",
    "\n",
    "* Fit the training data perfectly\n",
    "* But fail miserably on new houses\n",
    "* Because it's memorizing noise, not learning patterns\n",
    "\n",
    "### Solution: Add a Penalty with L2 Regularization\n",
    "\n",
    "Ridge Regression says: \"Make good predictions, BUT don't use huge numbers in your formula.\"\n",
    "\n",
    "* **The L2 penalty** is what keeps the numbers small and the model simple.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Benefits (Simple Terms)\n",
    "\n",
    "| Benefit                         | What it means                                    |\n",
    "| ------------------------------- | ------------------------------------------------ |\n",
    "| **Prevents Overfitting**        | Works better on new data                         |\n",
    "| **Handles Correlated Features** | Doesn't break when variables are related         |\n",
    "| **Stable Results**              | Small data changes don't cause big model changes |\n",
    "| **Works with Many Features**    | Handles datasets with lots of columns            |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Ridge Regression\n",
    "\n",
    "**Use it when:**\n",
    "\n",
    "* You have many features (columns)\n",
    "* Your features are correlated with each other\n",
    "* Regular regression gives you huge, unstable numbers\n",
    "* Your model works great on training data but poorly on test data\n",
    "\n",
    "**Don't use it when:**\n",
    "\n",
    "* You want to completely remove some features (use Lasso/L1 instead)\n",
    "* You have very few features and no overfitting problems\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Analogy\n",
    "\n",
    "Think of Ridge Regression like **driving with a speed limit**:\n",
    "\n",
    "* **Regular regression**: \"Get to your destination as fast as possible!\" (might crash)\n",
    "* **Ridge regression**: \"Get there fast, but don't exceed the speed limit!\" (safer journey)\n",
    "\n",
    "The **speed limit** is the **L2 penalty** — it keeps things under control.\n",
    "\n",
    "---\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "**Ridge Regression = Linear Regression + L2 Regularization (L2 penalty)**\n",
    "It prevents your model from going crazy by adding a smart penalty for complexity, thanks to L2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48bcaf",
   "metadata": {},
   "source": [
    "# Mathematical Example\n",
    "\n",
    "---\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge Regression is an improved version of linear regression that:\n",
    "\n",
    "- Fits a line through data points  \n",
    "- Keeps the model simple  \n",
    "- Adds a penalty to prevent large coefficient values  \n",
    "\n",
    "---\n",
    "\n",
    "## Ridge Regression Formula\n",
    "\n",
    "The cost function for Ridge Regression is:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - (w \\cdot x + b))^2 + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "For simplicity, we'll assume $ b = 0 $, so the equation becomes:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - w \\cdot x)^2 + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ n $: number of data points  \n",
    "- $ x_i, y_i $: input and actual output values  \n",
    "- $ w $: the coefficient (slope)  \n",
    "- $ \\lambda $: regularization strength  \n",
    "- $ w^2 $: penalty term that discourages large weights  \n",
    "\n",
    "---\n",
    "\n",
    "## Our Simple Data\n",
    "\n",
    "| x | y_actual |\n",
    "|---|----------|\n",
    "| 1 | 2        |\n",
    "| 2 | 3        |\n",
    "| 3 | 4        |\n",
    "\n",
    "We want to find the best value for $ w $ in:\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = w \\cdot x\n",
    "$$\n",
    "\n",
    "We will test different $ w $ values using:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\text{Average Error} + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "Assume:\n",
    "\n",
    "- $ \\lambda = 1 $\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Calculations\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 1 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 1 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error = $ y_{\\text{actual}} - y_{\\text{predicted}} $ | Error² |\n",
    "|---|----------|-------------|--------------------------------------------------------|--------|\n",
    "| 1 | 2        | 1           | 1                                                      | 1      |\n",
    "| 2 | 3        | 2           | 1                                                      | 1      |\n",
    "| 3 | 4        | 3           | 1                                                      | 1      |\n",
    "\n",
    "- Total error = $ 1 + 1 + 1 = 3 $\n",
    "- Average error = $ \\frac{3}{3} = 1 $\n",
    "- Penalty = $ 1^2 = 1 $\n",
    "- Total cost = $ 1 + 1 = \\mathbf{2} $\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 1.5 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 1.5 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error | Error² |\n",
    "|---|----------|-------------|--------|--------|\n",
    "| 1 | 2        | 1.5         | 0.5    | 0.25   |\n",
    "| 2 | 3        | 3.0         | 0      | 0      |\n",
    "| 3 | 4        | 4.5         | -0.5   | 0.25   |\n",
    "\n",
    "- Total error = $ 0.25 + 0 + 0.25 = 0.5 $\n",
    "- Average error = $ \\frac{0.5}{3} \\approx 0.167 $\n",
    "- Penalty = $ 1.5^2 = 2.25 $\n",
    "- Total cost = $ 0.167 + 2.25 = \\mathbf{2.417} $\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 0.8 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 0.8 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error | Error² |\n",
    "|---|----------|-------------|--------|--------|\n",
    "| 1 | 2        | 0.8         | 1.2    | 1.44   |\n",
    "| 2 | 3        | 1.6         | 1.4    | 1.96   |\n",
    "| 3 | 4        | 2.4         | 1.6    | 2.56   |\n",
    "\n",
    "- Total error = $ 1.44 + 1.96 + 2.56 = 5.96 $\n",
    "- Average error = $ \\frac{5.96}{3} \\approx 1.987 $\n",
    "- Penalty = $ 0.8^2 = 0.64 $\n",
    "- Total cost = $ 1.987 + 0.64 = \\mathbf{2.627} $\n",
    "\n",
    "---\n",
    "\n",
    "## Final Comparison Table\n",
    "\n",
    "| $ w $ | Average Error | Penalty $ w^2 $ | Total Cost     |\n",
    "|--------|----------------|--------------------|----------------|\n",
    "| 1.0    | 1.0            | 1.0                | **2.0 (Best)** |\n",
    "| 1.5    | 0.167          | 2.25               | 2.417          |\n",
    "| 0.8    | 1.987          | 0.64               | 2.627          |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Answer:\n",
    "\n",
    "The best value is:\n",
    "\n",
    "$$\n",
    "\\boxed{w = 1}\n",
    "\\quad \\Rightarrow \\quad y_{\\text{predicted}} = 1 \\cdot x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary:\n",
    "\n",
    "- Ridge Regression balances good predictions with small coefficients  \n",
    "- It uses this cost formula:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - wx)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "- We tested $ w = 1, 1.5, 0.8 $  \n",
    "- Best cost = **2** when $ w = 1 $\n",
    "\n",
    "So, Ridge Regression gives us a **simple, stable, and reliable model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a410f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90b7d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "X = np.array([1, 2, 3])      # input\n",
    "y = np.array([2, 3, 4])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "488d9f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([2, 3, 4]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fef21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalizing the values of w\n",
    "w_val = [0.8, 1.0, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dab426ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 1.0, 1.5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21bacc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 1  # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a830b536",
   "metadata": {},
   "outputs": [],
   "source": [
    " all_costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7efc3ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\t1.987\t\t0.640\t\t2.627\n",
      "1.0\t1.000\t\t1.000\t\t2.000\n",
      "1.5\t0.167\t\t2.250\t\t2.417\n"
     ]
    }
   ],
   "source": [
    "for w in w_val:\n",
    "    # Calculate predictions\n",
    "    y_pred = w * X\n",
    "    \n",
    "    # Calculate MSE (Mean Squared Error)\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate penalty (L2 regularization)\n",
    "    penalty = lambda_reg * (w ** 2)\n",
    "    \n",
    "    # Total cost\n",
    "    total_cost = mse + penalty\n",
    "    all_costs.append(total_cost)\n",
    "    \n",
    "    print(f\"{w}\\t{mse:.3f}\\t\\t{penalty:.3f}\\t\\t{total_cost:.3f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d429add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(2.626666666666666),\n",
       " np.float64(2.0),\n",
       " np.float64(2.4166666666666665)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc4c396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost= min(all_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e71f196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "002931d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weight_idx = all_costs.index(min_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b809fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weight_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "032d0f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weight = w_val[best_weight_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d10272e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best weight: w = 1.0 (lowest cost = 2.000)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest weight: w = {best_weight} (lowest cost = {min_cost:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
