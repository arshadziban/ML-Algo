{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395b0cf2",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge Regression is like **regular linear regression with a \"safety brake\"**. It prevents the model from getting too excited and fitting the data too closely.\n",
    "\n",
    "Think of it this way: \n",
    "- **Regular linear regression** tries to draw the perfect line through all data points\n",
    "- **Ridge regression** draws a good line but also keeps the line from getting too steep or wiggly\n",
    "\n",
    "## Simple Formula\n",
    "\n",
    "```\n",
    "Total Error = Prediction Error + Penalty for Big Numbers\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Prediction Error**: How wrong our predictions are\n",
    "- **Penalty**: A cost for having large coefficient values\n",
    "- **Big Numbers**: Large values in our model's weights/coefficients\n",
    "\n",
    "## Why Do We Need Ridge Regression?\n",
    "\n",
    "### Problem: Overfitting\n",
    "Imagine you're trying to predict house prices. Regular linear regression might:\n",
    "- Fit the training data perfectly \n",
    "- But fail miserably on new houses\n",
    "- Because it's memorizing noise, not learning patterns\n",
    "\n",
    "### Solution: Add a Penalty\n",
    "Ridge Regression says: \"Make good predictions, BUT don't use huge numbers in your formula.\"\n",
    "\n",
    "## Key Benefits (Simple Terms)\n",
    "\n",
    "| Benefit | What it means |\n",
    "|---------|---------------|\n",
    "| **Prevents Overfitting** | Works better on new data |\n",
    "| **Handles Correlated Features** | Doesn't break when variables are related |\n",
    "| **Stable Results** | Small data changes don't cause big model changes |\n",
    "| **Works with Many Features** | Handles datasets with lots of columns |\n",
    "\n",
    "## When to Use Ridge Regression\n",
    "\n",
    "**Use it when:**\n",
    "- You have many features (columns) \n",
    "- Your features are correlated with each other\n",
    "- Regular regression gives you huge, unstable numbers\n",
    "- Your model works great on training data but poorly on test data\n",
    "\n",
    "**Don't use it when:**\n",
    "- You want to completely remove some features (use Lasso instead)\n",
    "- You have very few features and no overfitting problems\n",
    "\n",
    "## Simple Analogy\n",
    "\n",
    "Think of Ridge Regression like **driving with a speed limit**:\n",
    "- **Regular regression**: \"Get to your destination as fast as possible!\" (might crash)\n",
    "- **Ridge regression**: \"Get there fast, but don't exceed the speed limit!\" (safer journey)\n",
    "\n",
    "The **speed limit** is like the penalty term - it keeps things under control.\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "Ridge Regression = Linear Regression + Common Sense  \n",
    "It prevents your model from going crazy by adding a simple penalty for complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48bcaf",
   "metadata": {},
   "source": [
    "# Mathematical Example\n",
    "\n",
    "---\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge Regression is an improved version of linear regression that:\n",
    "\n",
    "- Fits a line through data points  \n",
    "- Keeps the model simple  \n",
    "- Adds a penalty to prevent large coefficient values  \n",
    "\n",
    "---\n",
    "\n",
    "## Ridge Regression Formula\n",
    "\n",
    "The cost function for Ridge Regression is:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - (w \\cdot x + b))^2 + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "For simplicity, we'll assume $ b = 0 $, so the equation becomes:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - w \\cdot x)^2 + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ n $: number of data points  \n",
    "- $ x_i, y_i $: input and actual output values  \n",
    "- $ w $: the coefficient (slope)  \n",
    "- $ \\lambda $: regularization strength  \n",
    "- $ w^2 $: penalty term that discourages large weights  \n",
    "\n",
    "---\n",
    "\n",
    "## Our Simple Data\n",
    "\n",
    "| x | y_actual |\n",
    "|---|----------|\n",
    "| 1 | 2        |\n",
    "| 2 | 3        |\n",
    "| 3 | 4        |\n",
    "\n",
    "We want to find the best value for $ w $ in:\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = w \\cdot x\n",
    "$$\n",
    "\n",
    "We will test different $ w $ values using:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\text{Average Error} + \\lambda \\cdot w^2\n",
    "$$\n",
    "\n",
    "Assume:\n",
    "\n",
    "- $ \\lambda = 1 $\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Calculations\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 1 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 1 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error = $ y_{\\text{actual}} - y_{\\text{predicted}} $ | Error² |\n",
    "|---|----------|-------------|--------------------------------------------------------|--------|\n",
    "| 1 | 2        | 1           | 1                                                      | 1      |\n",
    "| 2 | 3        | 2           | 1                                                      | 1      |\n",
    "| 3 | 4        | 3           | 1                                                      | 1      |\n",
    "\n",
    "- Total error = $ 1 + 1 + 1 = 3 $\n",
    "- Average error = $ \\frac{3}{3} = 1 $\n",
    "- Penalty = $ 1^2 = 1 $\n",
    "- Total cost = $ 1 + 1 = \\mathbf{2} $\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 1.5 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 1.5 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error | Error² |\n",
    "|---|----------|-------------|--------|--------|\n",
    "| 1 | 2        | 1.5         | 0.5    | 0.25   |\n",
    "| 2 | 3        | 3.0         | 0      | 0      |\n",
    "| 3 | 4        | 4.5         | -0.5   | 0.25   |\n",
    "\n",
    "- Total error = $ 0.25 + 0 + 0.25 = 0.5 $\n",
    "- Average error = $ \\frac{0.5}{3} \\approx 0.167 $\n",
    "- Penalty = $ 1.5^2 = 2.25 $\n",
    "- Total cost = $ 0.167 + 2.25 = \\mathbf{2.417} $\n",
    "\n",
    "---\n",
    "\n",
    "### Try $ w = 0.8 $\n",
    "\n",
    "$$\n",
    "y_{\\text{predicted}} = 0.8 \\cdot x\n",
    "$$\n",
    "\n",
    "| x | y_actual | y_predicted | Error | Error² |\n",
    "|---|----------|-------------|--------|--------|\n",
    "| 1 | 2        | 0.8         | 1.2    | 1.44   |\n",
    "| 2 | 3        | 1.6         | 1.4    | 1.96   |\n",
    "| 3 | 4        | 2.4         | 1.6    | 2.56   |\n",
    "\n",
    "- Total error = $ 1.44 + 1.96 + 2.56 = 5.96 $\n",
    "- Average error = $ \\frac{5.96}{3} \\approx 1.987 $\n",
    "- Penalty = $ 0.8^2 = 0.64 $\n",
    "- Total cost = $ 1.987 + 0.64 = \\mathbf{2.627} $\n",
    "\n",
    "---\n",
    "\n",
    "## Final Comparison Table\n",
    "\n",
    "| $ w $ | Average Error | Penalty $ w^2 $ | Total Cost     |\n",
    "|--------|----------------|--------------------|----------------|\n",
    "| 1.0    | 1.0            | 1.0                | **2.0 (Best)** |\n",
    "| 1.5    | 0.167          | 2.25               | 2.417          |\n",
    "| 0.8    | 1.987          | 0.64               | 2.627          |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Answer:\n",
    "\n",
    "The best value is:\n",
    "\n",
    "$$\n",
    "\\boxed{w = 1}\n",
    "\\quad \\Rightarrow \\quad y_{\\text{predicted}} = 1 \\cdot x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary:\n",
    "\n",
    "- Ridge Regression balances good predictions with small coefficients  \n",
    "- It uses this cost formula:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = \\frac{1}{n} \\sum (y - wx)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "- We tested $ w = 1, 1.5, 0.8 $  \n",
    "- Best cost = **2** when $ w = 1 $\n",
    "\n",
    "So, Ridge Regression gives us a **simple, stable, and reliable model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a410f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b7d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "X = np.array([1, 2, 3])      # input\n",
    "y = np.array([2, 3, 4])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488d9f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([2, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef21f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
